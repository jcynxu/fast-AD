"""
训练循环：在线蒸馏流程
对应论文 3.5 节的 "Online Distillation Pipeline"
"""

import torch
import torch.nn as nn
import torch.optim as optim
from typing import Optional
import yaml

from models.diffusion_fast_ad import FastADGenerator, FastADConfig
from utils.buffer import ReplayBuffer
from utils.losses import BNLoss, KDLoss
from utils.logger import Logger
from utils.metrics import evaluate_model


class FastADTrainer:
    """
    Fast-AD 训练器：控制"合成-训练"的交替循环
    """
    def __init__(self, teacher: nn.Module, student: nn.Module, diffusion_model: nn.Module,
                 config: FastADConfig, device: str = 'cuda', num_classes: int = 100,
                 image_size: tuple = (32, 32), logger: Optional[Logger] = None):
        """
        Args:
            teacher: 预训练的 Teacher 模型
            student: 待训练的 Student 模型
            diffusion_model: 预训练的扩散模型
            config: FastADConfig 配置对象
            device: 计算设备
            num_classes: 类别数
            image_size: 图像尺寸 (H, W)
            logger: 日志记录器
        """
        self.teacher = teacher.to(device)
        self.student = student.to(device)
        self.diffusion = diffusion_model.to(device)
        self.config = config
        self.device = device
        self.num_classes = num_classes
        self.image_size = image_size
        self.logger = logger
        
        # 设置模型模式
        self.teacher.eval()
        self.diffusion.eval()
        self.student.train()
        
        # BN Loss 计算函数
        self.bn_loss_fn = BNLoss(self.teacher)
        
        # 初始化生成器和缓冲区
        self.generator = FastADGenerator(
            self.diffusion, self.teacher, config, device, image_size, self.bn_loss_fn
        )
        self.buffer = ReplayBuffer(max_size=config.buffer_size if hasattr(config, 'buffer_size') else 4096)
        
        # KD Loss (论文公式 9)
        # LKD = τ_kd² · KL(softmax(S(x)/τ_kd) || softmax(T(x)/τ_kd))
        kd_temp = getattr(config, 'kd_temperature', 4.0)  # 默认 4.0
        self.kd_loss_fn = KDLoss(temperature=kd_temp)
        
    def train_one_epoch(self, optimizer: optim.Optimizer, scheduler: Optional[optim.lr_scheduler._LRScheduler] = None,
                       synthesis_batch_size: int = 64, train_batch_size: int = 64,
                       train_iterations: int = 100, epoch: int = 0):
        """
        训练一个 epoch (在线蒸馏流程)
        
        论文描述：The overall training proceeds in an online manner involving a dynamic buffer.
        包含两个阶段：
        1. Synthesis Phase: 生成合成数据并推入 Buffer
        2. Distillation Phase: 从 Buffer 采样数据训练 Student
        
        Args:
            optimizer: Student 优化器
            scheduler: 学习率调度器
            synthesis_batch_size: 合成批次大小
            train_batch_size: 训练批次大小
            train_iterations: 每个 epoch 的训练迭代次数
            epoch: 当前 epoch
        """
        self.student.train()
        
        # --- 阶段 1: 数据合成 (Synthesis Phase) ---
        # 论文描述：In each synthesis cycle, a batch of rectified images x_syn 
        # generated by Fast-AD (via Eq. 7) is pushed into B.
        syn_targets = torch.randint(0, self.num_classes, (synthesis_batch_size,), device=self.device)
        
        # TODO: 这里可以接入 LLM 生成的 Prompts embedding
        # 论文描述：LLM-Driven Prompt Generation for diversity
        # prompts = get_llm_prompts(syn_targets)
        prompts_embeddings = None
        
        print(f"Epoch {epoch+1}: Synthesizing {synthesis_batch_size} images...")
        with torch.no_grad():
            # 使用 Fast-AD 生成修正图像 x_syn (通过公式 7)
            syn_images = self.generator.sample(synthesis_batch_size, syn_targets, prompts_embeddings)
        
        # Push: 将生成的图像推入 Buffer (FIFO 策略)
        self.buffer.push(syn_images, syn_targets)
        print(f"Buffer size: {len(self.buffer)}/{self.buffer.max_size}")
        
        # --- 阶段 2: 学生训练 (Distillation Phase / Student Update) ---
        # 论文描述：In the distillation phase, we sample a batch of synthetic images x ~ B
        # and update the student network S to minimize the KL Divergence with the teacher T.
        if len(self.buffer) < train_batch_size:
            print("Buffer not full enough, skipping training phase")
            return {'loss': 0.0, 'buffer_size': len(self.buffer)}
        
        total_loss = 0.0
        for iteration in range(train_iterations):
            # Sample: 从 Buffer 随机采样一批数据 x ~ B
            inputs, targets = self.buffer.sample(train_batch_size)
            inputs = inputs.to(self.device)
            targets = targets.to(self.device)
            
            # Forward: 获取 Teacher 和 Student 的 logits
            with torch.no_grad():
                teacher_logits = self.teacher(inputs)
            student_logits = self.student(inputs)
            
            # Loss: KL Divergence (论文公式 9)
            # LKD = τ_kd² · KL(softmax(S(x)/τ_kd) || softmax(T(x)/τ_kd))
            loss = self.kd_loss_fn(student_logits, teacher_logits)
            
            # Backward: 更新 Student 参数
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            if (iteration + 1) % 20 == 0:
                print(f"  Iteration {iteration+1}/{train_iterations}, Loss: {loss.item():.4f}")
        
        if scheduler is not None:
            scheduler.step()
        
        avg_loss = total_loss / train_iterations
        metrics = {
            'loss': avg_loss,
            'buffer_size': len(self.buffer),
            'lr': optimizer.param_groups[0]['lr']
        }
        
        if self.logger:
            self.logger.log(epoch + 1, metrics)
        else:
            print(f"Epoch {epoch+1}: Loss: {avg_loss:.4f}, Buffer Size: {len(self.buffer)}")
        
        return metrics
    
    def train(self, epochs: int, optimizer: optim.Optimizer,
              scheduler: Optional[optim.lr_scheduler._LRScheduler] = None,
              synthesis_batch_size: int = 64, train_batch_size: int = 64,
              train_iterations: int = 100, save_interval: int = 10):
        """
        完整训练流程
        
        Args:
            epochs: 训练轮数
            optimizer: Student 优化器
            scheduler: 学习率调度器
            synthesis_batch_size: 合成批次大小
            train_batch_size: 训练批次大小
            train_iterations: 每个 epoch 的训练迭代次数
            save_interval: 保存检查点的间隔
        """
        print("Starting Fast-AD Distillation...")
        print(f"Teacher: {type(self.teacher).__name__}")
        print(f"Student: {type(self.student).__name__}")
        print(f"Epochs: {epochs}, Buffer Size: {self.buffer.max_size}")
        
        for epoch in range(epochs):
            metrics = self.train_one_epoch(
                optimizer, scheduler, synthesis_batch_size,
                train_batch_size, train_iterations, epoch
            )
            
            # 保存检查点
            if (epoch + 1) % save_interval == 0 and self.logger:
                self.logger.save_checkpoint(self.student, optimizer, epoch + 1)
        
        print("Training completed!")
    
    def evaluate(self, dataloader: Optional[torch.utils.data.DataLoader] = None) -> dict:
        """
        评估 Student 模型性能
        
        Args:
            dataloader: 评估数据加载器 (可选)
            
        Returns:
            metrics: 评估指标
        """
        if dataloader is None:
            # 如果没有提供 dataloader，从 buffer 采样评估
            if len(self.buffer) == 0:
                return {'error': 'No data available for evaluation'}
            
            # 从 buffer 采样一批数据
            inputs, targets = self.buffer.sample(min(100, len(self.buffer)))
            inputs = inputs.to(self.device)
            targets = targets.to(self.device)
            
            self.student.eval()
            with torch.no_grad():
                logits = self.student(inputs)
                _, predicted = torch.max(logits.data, 1)
                correct = (predicted == targets).sum().item()
                total = targets.size(0)
                accuracy = 100.0 * correct / total
            
            return {'accuracy': accuracy, 'correct': correct, 'total': total}
        else:
            from utils.metrics import evaluate_model
            return evaluate_model(self.student, dataloader, self.device)


def train_fast_ad(teacher: nn.Module, student: nn.Module, diffusion_model: nn.Module,
                  config_path: Optional[str] = None, epochs: int = 200,
                  device: str = 'cuda', num_classes: int = 100, image_size: tuple = (32, 32),
                  log_dir: str = './logs'):
    """
    训练 Fast-AD 的主函数
    
    Args:
        teacher: 预训练的 Teacher 模型
        student: 待训练的 Student 模型
        diffusion_model: 预训练的扩散模型
        config_path: 配置文件路径
        epochs: 训练轮数
        device: 计算设备
        num_classes: 类别数
        image_size: 图像尺寸
        log_dir: 日志目录
    """
    # 加载配置
    if config_path:
        with open(config_path, 'r') as f:
            config_dict = yaml.safe_load(f)
        fast_ad_config = config_dict.get('fast_ad', {})
        distillation_config = config_dict.get('distillation', {})
        config = FastADConfig(fast_ad_config)
        config.buffer_size = distillation_config.get('buffer_size', 4096)
        config.kd_temperature = distillation_config.get('kd_temperature', 4.0)
    else:
        config = FastADConfig()
        config.buffer_size = 4096
    
    # 创建日志记录器
    logger = Logger(log_dir=log_dir)
    
    # 创建训练器
    trainer = FastADTrainer(
        teacher, student, diffusion_model, config, device,
        num_classes, image_size, logger
    )
    
    # 设置优化器和调度器
    optimizer = optim.SGD(
        student.parameters(),
        lr=distillation_config.get('lr', 0.1) if config_path else 0.1,
        momentum=0.9,
        weight_decay=5e-4
    )
    
    milestones = distillation_config.get('milestones', [100, 150]) if config_path else [100, 150]
    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)
    
    # 训练参数
    synthesis_batch_size = distillation_config.get('synthesis_batch_size', 64) if config_path else 64
    train_batch_size = distillation_config.get('train_batch_size', 64) if config_path else 64
    train_iterations = distillation_config.get('train_iterations', 100) if config_path else 100
    
    # 开始训练
    trainer.train(
        epochs=epochs,
        optimizer=optimizer,
        scheduler=scheduler,
        synthesis_batch_size=synthesis_batch_size,
        train_batch_size=train_batch_size,
        train_iterations=train_iterations
    )
    
    return trainer

