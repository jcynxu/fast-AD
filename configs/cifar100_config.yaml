# Fast-AD configuration for CIFAR-100

# Distillation settings
distillation:
  teacher_arch: "resnet34"          # Teacher architecture
  student_arch: "resnet18"          # Student architecture
  num_classes: 100                  # Number of classes (CIFAR-100)
  image_size: [32, 32]              # Image size [H, W]
  epochs: 200                       # Training epochs
  buffer_size: 4096                 # ReplayBuffer capacity
  lr: 0.1                           # Initial learning rate
  milestones: [100, 150]            # LR decay epochs
  synthesis_batch_size: 64          # Synthesized images per epoch
  train_batch_size: 64              # Student update batch size
  train_iterations: 100             # Student updates per epoch
  kd_temperature: 4.0               # Distillation temperature Ï„_kd (Eq. 9)
  
  # Optional pretrained checkpoints
  # teacher_checkpoint: "path/to/teacher.pth"
  # diffusion_checkpoint: "path/to/diffusion.pth"

# Fast-AD algorithm parameters
fast_ad:
  lambda_max: 1.5                   # Maximum rectification strength
  eta: 0.1                          # Gradient scaling factor
  tau_ent: 0.4                      # Entropy threshold (CIFAR-100: 0.4, ImageNet: 0.6)
  k_sigmoid: 10.0                   # Sigmoid steepness
  ddim_steps: 50                    # Number of DDIM steps (~20x speedup vs 1000-step DDPM)
  xi: 1e-6                          # Epsilon for numerical stability in grad norm
  gamma: 1.0                        # CE loss weight
  T: 1000                           # Original diffusion timesteps

